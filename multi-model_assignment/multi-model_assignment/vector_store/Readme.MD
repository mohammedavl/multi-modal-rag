Multi-Modal Document Intelligence (RAG-Based QA System)

A complete end-to-end Retrieval-Augmented Generation pipeline designed to process real-world IMF Article IV reports containing text, tables, charts, and scanned images â€” and answer questions with LLM-generated, citation-backed responses.

ğŸ“Œ Project Overview

Modern financial and policy documents are multi-modal in nature.
This project builds a multi-modal RAG system capable of:

Extracting text, tables, and images (with OCR)

Converting them into structured chunks

Generating embeddings for all modalities

Storing them in a vector DB (Chroma)

Performing similarity-based retrieval

Answering questions using an LLM with proper attribution

This is a fully functional prototype aligned with real enterprise document-intelligence systems.

ğŸ— System Architecture
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚    PDF File    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                  Multi-Modal Ingestion
                            â”‚
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚           â”‚          â”‚           â”‚
   Text Chunks   Table Rows   OCR Images   Metadata
       â”‚           â”‚          â”‚           â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 Chunking & Cleaning
                            â”‚
                  Embeddings (MiniLM)
                            â”‚
                 Chroma Vector Database
                            â”‚
                Retrieval-Augmented QA
                            â”‚
              Streamlit Chatbot + Citations

âœ¨ Features
ğŸ” Multi-Modal Document Ingestion

Text extraction (PyMuPDF)

Table extraction (Camelot)

Image extraction + OCR (Tesseract)

Per-page chunking and metadata tagging

ğŸ§  Vector Indexing

Sentence-Transformers MiniLM

Unified multi-modal embedding space

Stored in ChromaDB (chroma.sqlite3 + chunks.pkl)

ğŸ¤– Question Answering

Similarity search

LLM answer generation (Flan-T5 or fallback SimpleQA)

Citation-backed responses

Page-level attribution

ğŸ’¬ Interactive Streamlit App

Chat interface

Retrieval visualization

Chunk statistics

Clear document insights

ğŸ“¦ Project Structure

multi-model-assignment/
â”‚
â”œâ”€â”€ app.py                     # Streamlit UI
â”œâ”€â”€ process_document.py        # Multi-modal ingestion
â”œâ”€â”€ create_embeddings.py       # Embedding generation
â”œâ”€â”€ vector_store.py            # Chroma store wrapper
â”œâ”€â”€ llm_qa.py                  # RAG + citations
â”œâ”€â”€ config.py                  # Directories & model configs
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                   # Input PDFs
â”‚   â”œâ”€â”€ images/                # Extracted images
â”‚   â”œâ”€â”€ processed/             # Extracted chunks (JSON)
â”‚   â””â”€â”€ vector_store/          # Chroma files + chunks.pkl
â”‚
â””â”€â”€ requirements.txt

âš™ï¸ Installation
1. Clone the Repository
git clone <your-repo-url>
cd multi-model_assignment

2. Install Dependencies
pip install -r requirements.txt

3. Install Tesseract (For OCR)

Download from:
https://github.com/tesseract-ocr/tesseract

â–¶ï¸ How to Run the Project
Step 1 â€” Place PDF in data/raw/
data/raw/qatar_test_doc.pdf

Step 2 â€” Run Document Processing
python process_document.py

Step 3 â€” Create Embeddings
python create_embeddings.py

Step 4 â€” Run Streamlit App
python -m streamlit run app.py


You should see:

âœ” Ready to use
âœ” Total chunks loaded
âœ” Text, table, image counts
âœ” Chat interface active

ğŸ§ª Example Queries
Type	Sample Question
Text	â€œSummarize Qatarâ€™s economic outlook.â€
Tables	â€œWhat does the table on page 43 show?â€
OCR Images	â€œWhat text was extracted from scanned images?â€
Cross-Modal	â€œCompare inflation values from text and tables.â€
ğŸ§  Future Improvements (Excellence Track)

Cross-modal reranking using CLIP

Hybrid search: RRF fusion (BM25 + dense vectors)

Structured KPI extraction

Analytics dashboard (latency, recall charts)

Long-context LLM integration (Llama-3-Long, GPT-4.1-1106)

ğŸ¥ Video Demonstration

A 3â€“5 minute video script is included covering:

Pipeline explanation

Code walkthrough

Application demo

Insights

This satisfies the assignmentâ€™s mandatory video requirement.

ğŸ‘¤ Author

Mohammed
Multi-Modal RAG Engineer
Big AIR Lab Candidate Submission